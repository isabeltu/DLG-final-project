{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81387f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from model import Generator\n",
    "from model import Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d495b2b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes found: 325\n",
      "Classes: ['Bird', 'airplane', 'antenna', 'apron', 'archbishop', 'armchair', 'armor', 'arrow', 'art', 'artist', 'assassin', 'author', 'automobile', 'avocado', 'ax', 'axe', 'bag', 'ball', 'ballerina', 'balloon', 'bandage', 'banner', 'barge', 'bark', 'barrel', 'baseball', 'basin', 'basket', 'bathrobe', 'bathtub', 'baton', 'bayonet', 'bed', 'bedding', 'bedroom', 'bell', 'belt', 'bench', 'bicycle', 'bin', 'blackboard', 'blade', 'blanket', 'bludgeon', 'boat', 'bonnet', 'book', 'boot', 'bottle', 'bouquet', 'bow', 'bowl', 'box', 'brick', 'bride', 'briefcase', 'broom', 'brush', 'bucket', 'buckle', 'bugle', 'bulb', 'button', 'cabinet', 'cage', 'cake', 'camera', 'can', 'candle', 'cane', 'cannon', 'canvas', 'capacitor', 'carbine', 'carpet', 'carriage', 'cart', 'cartridge', 'carving', 'chair', 'chandelier', 'charcoal', 'chisel', 'clip', 'clock', 'coat', 'column', 'comb', 'compass', 'costume', 'couch', 'cowboy', 'crib', 'crutch', 'cup', 'cupboard', 'curtain', 'cyclist', 'dancer', 'desk', 'disc', 'dish', 'dog', 'drawer', 'drum', 'dugout', 'easel', 'electrode', 'emblem', 'envelope', 'fan', 'farmer', 'feather', 'firearm', 'fireplace', 'fisherman', 'flag', 'flannel', 'flashlight', 'flower', 'foil', 'football', 'fork', 'frieze', 'gardener', 'gatepost', 'glass', 'glove', 'gown', 'grandmother', 'grape', 'greatcoat', 'guitar', 'gun', 'hair', 'hammer', 'hamper', 'handbag', 'handkerchief', 'harness', 'hat', 'headlight', 'hinge', 'holster', 'horn', 'horse', 'horseman', 'icbm', 'instrument', 'jacket', 'jar', 'jeep', 'key', 'knapsack', 'knife', 'knight', 'knob', 'ladder', 'lady', 'lamp', 'lantern', 'lemon', 'lens', 'luggage', 'magazine', 'mantel', 'map', 'marksman', 'mast', 'masterpiece', 'melon', 'micrometeorite', 'microphone', 'mirror', 'mistress', 'monk', 'monument', 'mortar', 'mosaic', 'motor', 'musician', 'musket', 'native', 'necktie', 'needle', 'newspaper', 'notebook', 'nut', 'orange', 'overcoat', 'package', 'pail', 'paint', 'painter', 'paper', 'parachute', 'patch', 'pear', 'peasant', 'pebble', 'pen', 'pencil', 'performer', 'person', 'pianist', 'piano', 'pillow', 'pipe', 'pitcher', 'plane', 'plank', 'planner', 'planter', 'plaque', 'plaster', 'plate', 'plow', 'pole', 'poncho', 'portrait', 'pot', 'pottery', 'preacher', 'priest', 'prisoner', 'publication', 'purse', 'pyrometer', 'quack', 'quarters', 'raft', 'razor', 'reader', 'recorder', 'redcoat', 'reflection', 'refrigerator', 'revolver', 'rifle', 'rifleman', 'ring', 'robe', 'rock', 'rug', 'saddle', 'saddlebag', 'sandal', 'scale', 'scarf', 'seat', 'seismograph', 'shackle', 'shawl', 'shell', 'shield', 'ship', 'shirt', 'shoe', 'shotgun', 'shovel', 'silo', 'sink', 'sioux', 'skirt', 'slave', 'sleeve', 'slipper', 'slugger', 'sock', 'sofa', 'sovereign', 'speaker', 'sphere', 'spire', 'sputnik', 'staff', 'stairs', 'stairway', 'statue', 'stick', 'stirrup', 'stocking', 'stool', 'strap', 'streetlight', 'study', 'suit', 'suitcase', 'swatch', 'sweatshirt', 'sword', 'table', 'tarpaulin', 'tent', 'thread', 'toilet', 'torch', 'towel', 'tower', 'toy', 'tractor', 'tray', 'trouser', 'truck', 'turban', 'umbrella', 'undershirt', 'utensil', 'vase', 'vehicle', 'veil', 'vent', 'vest', 'violin', 'wagon', 'wallet', 'wastebasket', 'watch', 'weapon', 'weave', 'wheel', 'widow', 'wildcat', 'witch', 'wreath']\n"
     ]
    }
   ],
   "source": [
    "# sort cropped images\n",
    "\n",
    "input_dir = 'mask_crops'\n",
    "output_dir = 'sorted_crops'\n",
    "size = (128, 128) \n",
    "\n",
    "\n",
    "filenames = [f for f in os.listdir(input_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "labels = [os.path.splitext(f)[0].rsplit('_', 1)[1] for f in filenames]\n",
    "classes = sorted(set(labels))\n",
    "class_to_idx = {c: i for i, c in enumerate(classes)}\n",
    "\n",
    "for c in classes:\n",
    "    os.makedirs(os.path.join(output_dir, c), exist_ok=True)\n",
    "\n",
    "for f, label in zip(filenames, labels):\n",
    "    img = Image.open(os.path.join(input_dir, f)).convert('RGB')\n",
    "    img = img.resize(size, Image.LANCZOS)\n",
    "    save_path = os.path.join(output_dir, label, f)\n",
    "    img.save(save_path)\n",
    "\n",
    "file_paths = [os.path.join(label, f) for f, label in zip(filenames, labels)]\n",
    "one_hot = np.zeros((len(file_paths), len(classes)), dtype=np.uint8)\n",
    "for i, label in enumerate(labels):\n",
    "    one_hot[i, class_to_idx[label]] = 1\n",
    "\n",
    "np.save(os.path.join(output_dir, 'classes.npy'), np.array(classes))\n",
    "np.save(os.path.join(output_dir, 'labels.npy'), one_hot)\n",
    "with open(os.path.join(output_dir, 'file_paths.json'), 'w') as fp:\n",
    "    json.dump(file_paths, fp, indent=2)\n",
    "with open(os.path.join(output_dir, 'class_to_idx.json'), 'w') as fp:\n",
    "    json.dump(class_to_idx, fp, indent=2)\n",
    "\n",
    "print(\"Classes found:\", len(classes))\n",
    "print(\"Classes:\", classes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3eb400",
   "metadata": {},
   "outputs": [],
   "source": [
    "object_folder = \"person\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee2f1b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# augment data if needed\n",
    "\n",
    "sorted_crops_dir = \"sorted_crops\"\n",
    "\n",
    "def augment_folder(folder_path, target_count=1000):\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "    files = [f for f in os.listdir(folder_path)]\n",
    "    count = len(files)\n",
    "    idx = 0\n",
    "    while count < target_count:\n",
    "\n",
    "        src_name = files[np.random.randint(0, len(files))]\n",
    "        src_path = os.path.join(folder_path, src_name)\n",
    "        img = cv2.imread(src_path)\n",
    "        if img is None:\n",
    "            continue\n",
    "\n",
    "        h, w = img.shape[:2]\n",
    "\n",
    "        if np.random.rand() < 0.5:\n",
    "            img = cv2.flip(img, 1)\n",
    "\n",
    "        h,w = img.shape[:2]\n",
    "        rand_crop = np.random.randint(int(h*0.7), h)\n",
    "        x = np.random.randint(0, w - rand_crop + 1)\n",
    "        y = np.random.randint(0, h - rand_crop + 1)\n",
    "        img_aug = img[y:y+rand_crop, x:x+rand_crop]\n",
    "        img_aug = cv2.resize(img_aug, (w, h))\n",
    "\n",
    "        scale = np.random.uniform(0.6, 1.0)\n",
    "        img_aug = np.clip(img_aug * scale, 0, 255).astype(np.uint8)\n",
    "\n",
    "        base, ext = os.path.splitext(src_name)\n",
    "        new_name = f\"aug_{base}_{idx:04d}{ext}\"\n",
    "        save_path = os.path.join(folder_path, new_name)\n",
    "\n",
    "        cv2.imwrite(save_path, img_aug)\n",
    "        idx += 1\n",
    "        count += 1\n",
    "\n",
    "data_root = os.path.join(sorted_crops_dir, object_folder)\n",
    "augment_folder(data_root, target_count=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db157c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/200: 100%|██████████| 16/16 [00:07<00:00,  2.16batch/s, lossD=1.3070, lossG=-0.3602]\n",
      "Epoch 2/200: 100%|██████████| 16/16 [00:07<00:00,  2.15batch/s, lossD=1.3621, lossG=-0.0583]\n",
      "Epoch 3/200: 100%|██████████| 16/16 [00:07<00:00,  2.14batch/s, lossD=1.2860, lossG=-0.0064]\n",
      "Epoch 4/200: 100%|██████████| 16/16 [00:07<00:00,  2.13batch/s, lossD=1.3379, lossG=0.1056] \n",
      "Epoch 5/200: 100%|██████████| 16/16 [00:07<00:00,  2.12batch/s, lossD=1.4781, lossG=-0.0855]\n",
      "Epoch 6/200: 100%|██████████| 16/16 [00:07<00:00,  2.11batch/s, lossD=1.1644, lossG=0.5010] \n",
      "Epoch 7/200: 100%|██████████| 16/16 [00:07<00:00,  2.12batch/s, lossD=1.1134, lossG=0.8380] \n",
      "Epoch 8/200: 100%|██████████| 16/16 [00:07<00:00,  2.11batch/s, lossD=2.1786, lossG=0.1527]\n",
      "Epoch 9/200: 100%|██████████| 16/16 [00:07<00:00,  2.11batch/s, lossD=1.3094, lossG=-0.1146]\n",
      "Epoch 10/200: 100%|██████████| 16/16 [00:07<00:00,  2.10batch/s, lossD=1.5983, lossG=-0.3164]\n",
      "Epoch 11/200: 100%|██████████| 16/16 [00:07<00:00,  2.10batch/s, lossD=1.1989, lossG=0.6199] \n",
      "Epoch 12/200: 100%|██████████| 16/16 [00:07<00:00,  2.09batch/s, lossD=1.3741, lossG=-0.0288]\n",
      "Epoch 13/200: 100%|██████████| 16/16 [00:07<00:00,  2.10batch/s, lossD=1.4254, lossG=0.1614] \n",
      "Epoch 14/200: 100%|██████████| 16/16 [00:07<00:00,  2.09batch/s, lossD=1.5863, lossG=-0.4689]\n",
      "Epoch 15/200: 100%|██████████| 16/16 [00:07<00:00,  2.09batch/s, lossD=1.5553, lossG=-0.4456]\n",
      "Epoch 16/200: 100%|██████████| 16/16 [00:07<00:00,  2.09batch/s, lossD=1.8674, lossG=-0.1643]\n",
      "Epoch 17/200: 100%|██████████| 16/16 [00:07<00:00,  2.08batch/s, lossD=1.7204, lossG=-0.2318]\n",
      "Epoch 18/200: 100%|██████████| 16/16 [00:07<00:00,  2.08batch/s, lossD=1.8358, lossG=0.6068] \n",
      "Epoch 19/200: 100%|██████████| 16/16 [00:07<00:00,  2.08batch/s, lossD=1.6382, lossG=2.3074] \n",
      "Epoch 20/200: 100%|██████████| 16/16 [00:07<00:00,  2.08batch/s, lossD=2.0485, lossG=0.9928] \n",
      "Epoch 21/200: 100%|██████████| 16/16 [00:07<00:00,  2.08batch/s, lossD=1.8853, lossG=0.7274] \n",
      "Epoch 22/200: 100%|██████████| 16/16 [00:07<00:00,  2.08batch/s, lossD=1.8468, lossG=0.3371] \n",
      "Epoch 23/200: 100%|██████████| 16/16 [00:07<00:00,  2.07batch/s, lossD=1.6239, lossG=1.6794] \n",
      "Epoch 24/200: 100%|██████████| 16/16 [00:07<00:00,  2.08batch/s, lossD=1.9671, lossG=0.2463] \n",
      "Epoch 25/200: 100%|██████████| 16/16 [00:07<00:00,  2.08batch/s, lossD=1.9370, lossG=0.3066]\n",
      "Epoch 26/200: 100%|██████████| 16/16 [00:07<00:00,  2.07batch/s, lossD=2.0216, lossG=0.3914] \n",
      "Epoch 27/200: 100%|██████████| 16/16 [00:07<00:00,  2.07batch/s, lossD=2.0071, lossG=0.0590] \n",
      "Epoch 28/200: 100%|██████████| 16/16 [00:07<00:00,  2.07batch/s, lossD=1.8793, lossG=0.2387]\n",
      "Epoch 29/200: 100%|██████████| 16/16 [00:07<00:00,  2.07batch/s, lossD=1.7884, lossG=0.0907] \n",
      "Epoch 30/200: 100%|██████████| 16/16 [00:07<00:00,  2.07batch/s, lossD=1.8498, lossG=0.4692] \n",
      "Epoch 31/200: 100%|██████████| 16/16 [00:07<00:00,  2.06batch/s, lossD=2.0028, lossG=0.1056] \n",
      "Epoch 32/200: 100%|██████████| 16/16 [00:07<00:00,  2.08batch/s, lossD=2.0093, lossG=0.2865] \n",
      "Epoch 33/200: 100%|██████████| 16/16 [00:07<00:00,  2.07batch/s, lossD=1.9717, lossG=0.0872] \n",
      "Epoch 34/200: 100%|██████████| 16/16 [00:07<00:00,  2.07batch/s, lossD=1.9686, lossG=0.2527] \n",
      "Epoch 35/200: 100%|██████████| 16/16 [00:07<00:00,  2.07batch/s, lossD=2.0009, lossG=0.0901] \n",
      "Epoch 36/200: 100%|██████████| 16/16 [00:07<00:00,  2.07batch/s, lossD=1.9874, lossG=0.1668] \n",
      "Epoch 37/200: 100%|██████████| 16/16 [00:07<00:00,  2.07batch/s, lossD=2.0116, lossG=0.0672] \n",
      "Epoch 38/200: 100%|██████████| 16/16 [00:07<00:00,  2.07batch/s, lossD=2.0033, lossG=0.0812]\n",
      "Epoch 39/200: 100%|██████████| 16/16 [00:07<00:00,  2.07batch/s, lossD=2.0014, lossG=-0.0419]\n",
      "Epoch 40/200: 100%|██████████| 16/16 [00:07<00:00,  2.07batch/s, lossD=2.0023, lossG=0.2036]\n",
      "Epoch 41/200: 100%|██████████| 16/16 [00:07<00:00,  2.07batch/s, lossD=2.0200, lossG=0.0360]\n",
      "Epoch 42/200: 100%|██████████| 16/16 [00:07<00:00,  2.07batch/s, lossD=2.0079, lossG=0.1220] \n",
      "Epoch 43/200: 100%|██████████| 16/16 [00:07<00:00,  2.07batch/s, lossD=2.0515, lossG=0.1264] \n",
      "Epoch 44/200: 100%|██████████| 16/16 [00:07<00:00,  2.07batch/s, lossD=1.9853, lossG=0.0981]\n",
      "Epoch 45/200: 100%|██████████| 16/16 [00:07<00:00,  2.07batch/s, lossD=2.0060, lossG=0.0699]\n",
      "Epoch 46/200: 100%|██████████| 16/16 [00:07<00:00,  2.07batch/s, lossD=2.0020, lossG=0.0747]\n",
      "Epoch 47/200: 100%|██████████| 16/16 [00:07<00:00,  2.07batch/s, lossD=1.9918, lossG=-0.0153]\n",
      "Epoch 48/200: 100%|██████████| 16/16 [00:07<00:00,  2.07batch/s, lossD=2.0004, lossG=0.0884]\n",
      "Epoch 49/200: 100%|██████████| 16/16 [00:07<00:00,  2.07batch/s, lossD=1.9876, lossG=0.1618] \n",
      "Epoch 50/200: 100%|██████████| 16/16 [00:07<00:00,  2.07batch/s, lossD=2.0214, lossG=0.2438]\n",
      "Epoch 51/200: 100%|██████████| 16/16 [00:07<00:00,  2.07batch/s, lossD=1.9845, lossG=0.0979]\n",
      "Epoch 52/200: 100%|██████████| 16/16 [00:07<00:00,  2.07batch/s, lossD=2.0028, lossG=0.1620] \n",
      "Epoch 53/200: 100%|██████████| 16/16 [00:07<00:00,  2.07batch/s, lossD=2.0029, lossG=-0.0191]\n",
      "Epoch 54/200: 100%|██████████| 16/16 [00:07<00:00,  2.07batch/s, lossD=2.0251, lossG=0.1120]\n",
      "Epoch 55/200: 100%|██████████| 16/16 [00:07<00:00,  2.07batch/s, lossD=1.9852, lossG=0.1685]\n",
      "Epoch 56/200: 100%|██████████| 16/16 [00:07<00:00,  2.07batch/s, lossD=1.9874, lossG=-0.0284]\n",
      "Epoch 57/200: 100%|██████████| 16/16 [00:07<00:00,  2.07batch/s, lossD=1.9872, lossG=0.3061]\n",
      "Epoch 58/200: 100%|██████████| 16/16 [00:07<00:00,  2.07batch/s, lossD=2.0197, lossG=0.2836]\n",
      "Epoch 59/200: 100%|██████████| 16/16 [00:07<00:00,  2.07batch/s, lossD=1.9818, lossG=0.2973]\n",
      "Epoch 60/200: 100%|██████████| 16/16 [00:07<00:00,  2.07batch/s, lossD=1.9986, lossG=0.0984]\n",
      "Epoch 61/200: 100%|██████████| 16/16 [00:07<00:00,  2.07batch/s, lossD=1.9392, lossG=0.4063] \n",
      "Epoch 62/200: 100%|██████████| 16/16 [00:07<00:00,  2.07batch/s, lossD=1.9854, lossG=0.4625] \n",
      "Epoch 63/200: 100%|██████████| 16/16 [00:07<00:00,  2.07batch/s, lossD=2.0542, lossG=0.5331] \n",
      "Epoch 64/200: 100%|██████████| 16/16 [00:07<00:00,  2.07batch/s, lossD=1.9739, lossG=0.7219]\n",
      "Epoch 65/200: 100%|██████████| 16/16 [00:07<00:00,  2.07batch/s, lossD=1.9828, lossG=0.1713] \n",
      "Epoch 66/200: 100%|██████████| 16/16 [00:07<00:00,  2.07batch/s, lossD=2.0066, lossG=0.2670] \n",
      "Epoch 67/200: 100%|██████████| 16/16 [00:07<00:00,  2.07batch/s, lossD=1.9378, lossG=0.2340]\n",
      "Epoch 68/200: 100%|██████████| 16/16 [00:07<00:00,  2.07batch/s, lossD=2.0138, lossG=0.7233]\n",
      "Epoch 69/200: 100%|██████████| 16/16 [00:07<00:00,  2.07batch/s, lossD=1.9365, lossG=0.3071]\n",
      "Epoch 70/200: 100%|██████████| 16/16 [00:07<00:00,  2.07batch/s, lossD=1.9740, lossG=0.2415]\n",
      "Epoch 71/200: 100%|██████████| 16/16 [00:07<00:00,  2.07batch/s, lossD=2.0103, lossG=0.2748] \n",
      "Epoch 72/200: 100%|██████████| 16/16 [00:07<00:00,  2.07batch/s, lossD=1.8566, lossG=0.1081]\n",
      "Epoch 73/200: 100%|██████████| 16/16 [00:07<00:00,  2.07batch/s, lossD=1.9971, lossG=0.0028] \n",
      "Epoch 74/200: 100%|██████████| 16/16 [00:07<00:00,  2.07batch/s, lossD=1.9696, lossG=0.1508] \n",
      "Epoch 75/200: 100%|██████████| 16/16 [00:07<00:00,  2.07batch/s, lossD=1.8734, lossG=0.3588] \n",
      "Epoch 76/200: 100%|██████████| 16/16 [00:07<00:00,  2.07batch/s, lossD=1.8953, lossG=0.0370]\n",
      "Epoch 77/200: 100%|██████████| 16/16 [00:07<00:00,  2.07batch/s, lossD=1.9242, lossG=0.3958]\n",
      "Epoch 78/200: 100%|██████████| 16/16 [00:07<00:00,  2.07batch/s, lossD=1.8838, lossG=0.5668]\n",
      "Epoch 79/200: 100%|██████████| 16/16 [00:07<00:00,  2.07batch/s, lossD=1.9859, lossG=0.4882]\n",
      "Epoch 80/200: 100%|██████████| 16/16 [00:07<00:00,  2.08batch/s, lossD=1.9523, lossG=0.3147]\n",
      "Epoch 81/200: 100%|██████████| 16/16 [00:07<00:00,  2.07batch/s, lossD=1.8220, lossG=0.6772] \n",
      "Epoch 82/200: 100%|██████████| 16/16 [00:07<00:00,  2.07batch/s, lossD=1.9432, lossG=0.5461]\n",
      "Epoch 83/200: 100%|██████████| 16/16 [00:07<00:00,  2.07batch/s, lossD=1.8821, lossG=0.3642]\n",
      "Epoch 84/200: 100%|██████████| 16/16 [00:07<00:00,  2.06batch/s, lossD=1.8885, lossG=0.2625] \n",
      "Epoch 85/200: 100%|██████████| 16/16 [00:07<00:00,  2.07batch/s, lossD=2.7769, lossG=-0.1393]\n",
      "Epoch 86/200: 100%|██████████| 16/16 [00:07<00:00,  2.07batch/s, lossD=2.0147, lossG=0.9394]\n",
      "Epoch 87/200: 100%|██████████| 16/16 [00:07<00:00,  2.07batch/s, lossD=1.8013, lossG=0.9940]\n",
      "Epoch 88/200: 100%|██████████| 16/16 [00:07<00:00,  2.07batch/s, lossD=1.8676, lossG=1.0720]\n",
      "Epoch 89/200: 100%|██████████| 16/16 [00:07<00:00,  2.07batch/s, lossD=1.9335, lossG=0.4804]\n",
      "Epoch 90/200: 100%|██████████| 16/16 [00:07<00:00,  2.07batch/s, lossD=1.7371, lossG=-0.0601]\n",
      "Epoch 91/200: 100%|██████████| 16/16 [00:07<00:00,  2.07batch/s, lossD=1.8130, lossG=0.3419]\n",
      "Epoch 92/200: 100%|██████████| 16/16 [00:07<00:00,  2.07batch/s, lossD=1.8702, lossG=0.6006]\n",
      "Epoch 93/200: 100%|██████████| 16/16 [00:07<00:00,  2.07batch/s, lossD=1.8876, lossG=0.6318] \n",
      "Epoch 94/200: 100%|██████████| 16/16 [00:07<00:00,  2.07batch/s, lossD=1.7379, lossG=0.5632] \n",
      "Epoch 95/200: 100%|██████████| 16/16 [00:07<00:00,  2.07batch/s, lossD=1.6417, lossG=0.6214] \n",
      "Epoch 96/200: 100%|██████████| 16/16 [00:07<00:00,  2.07batch/s, lossD=1.9696, lossG=1.5636] \n",
      "Epoch 97/200: 100%|██████████| 16/16 [00:07<00:00,  2.07batch/s, lossD=1.8629, lossG=0.4172] \n",
      "Epoch 98/200: 100%|██████████| 16/16 [00:07<00:00,  2.07batch/s, lossD=1.7753, lossG=0.5047] \n",
      "Epoch 99/200: 100%|██████████| 16/16 [00:07<00:00,  2.07batch/s, lossD=1.7350, lossG=0.0078]\n",
      "Epoch 100/200: 100%|██████████| 16/16 [00:07<00:00,  2.07batch/s, lossD=1.7271, lossG=0.8937] \n",
      "Epoch 101/200: 100%|██████████| 16/16 [00:07<00:00,  2.07batch/s, lossD=1.8252, lossG=0.5291] \n",
      "Epoch 102/200: 100%|██████████| 16/16 [00:07<00:00,  2.07batch/s, lossD=1.6720, lossG=1.1009]\n",
      "Epoch 103/200: 100%|██████████| 16/16 [00:07<00:00,  2.07batch/s, lossD=1.6422, lossG=0.9478] \n",
      "Epoch 104/200: 100%|██████████| 16/16 [00:07<00:00,  2.08batch/s, lossD=1.6618, lossG=0.7934]\n",
      "Epoch 105/200: 100%|██████████| 16/16 [00:07<00:00,  2.08batch/s, lossD=1.7336, lossG=0.5538] \n",
      "Epoch 106/200: 100%|██████████| 16/16 [00:07<00:00,  2.08batch/s, lossD=1.6134, lossG=0.8536]\n",
      "Epoch 107/200: 100%|██████████| 16/16 [00:07<00:00,  2.08batch/s, lossD=1.6537, lossG=0.6031] \n",
      "Epoch 108/200: 100%|██████████| 16/16 [00:07<00:00,  2.07batch/s, lossD=1.6862, lossG=-0.2897]\n",
      "Epoch 109/200: 100%|██████████| 16/16 [00:07<00:00,  2.07batch/s, lossD=1.5624, lossG=0.5408]\n",
      "Epoch 110/200: 100%|██████████| 16/16 [00:07<00:00,  2.08batch/s, lossD=1.3722, lossG=1.0434] \n",
      "Epoch 111/200: 100%|██████████| 16/16 [00:07<00:00,  2.07batch/s, lossD=1.5289, lossG=1.1891]\n",
      "Epoch 112/200: 100%|██████████| 16/16 [00:07<00:00,  2.08batch/s, lossD=1.4572, lossG=1.1291]\n",
      "Epoch 113/200: 100%|██████████| 16/16 [00:07<00:00,  2.08batch/s, lossD=1.8029, lossG=0.5987]\n",
      "Epoch 114/200: 100%|██████████| 16/16 [00:07<00:00,  2.08batch/s, lossD=1.4703, lossG=0.7185] \n",
      "Epoch 115/200: 100%|██████████| 16/16 [00:07<00:00,  2.08batch/s, lossD=1.4379, lossG=1.6340] \n",
      "Epoch 116/200: 100%|██████████| 16/16 [00:07<00:00,  2.08batch/s, lossD=1.3574, lossG=0.9258] \n",
      "Epoch 117/200: 100%|██████████| 16/16 [00:07<00:00,  2.08batch/s, lossD=1.4832, lossG=0.4942]\n",
      "Epoch 118/200: 100%|██████████| 16/16 [00:07<00:00,  2.08batch/s, lossD=1.5140, lossG=0.8924] \n",
      "Epoch 119/200: 100%|██████████| 16/16 [00:07<00:00,  2.08batch/s, lossD=1.2916, lossG=1.1639] \n",
      "Epoch 120/200: 100%|██████████| 16/16 [00:07<00:00,  2.08batch/s, lossD=1.5511, lossG=0.7087]\n",
      "Epoch 121/200: 100%|██████████| 16/16 [00:07<00:00,  2.08batch/s, lossD=1.2878, lossG=1.0448]\n",
      "Epoch 122/200: 100%|██████████| 16/16 [00:07<00:00,  2.08batch/s, lossD=1.4301, lossG=0.9655] \n",
      "Epoch 123/200: 100%|██████████| 16/16 [00:07<00:00,  2.08batch/s, lossD=1.5132, lossG=1.0304] \n",
      "Epoch 124/200: 100%|██████████| 16/16 [00:07<00:00,  2.08batch/s, lossD=1.3764, lossG=0.8284] \n",
      "Epoch 125/200: 100%|██████████| 16/16 [00:07<00:00,  2.08batch/s, lossD=1.4572, lossG=0.8753] \n",
      "Epoch 126/200: 100%|██████████| 16/16 [00:07<00:00,  2.08batch/s, lossD=1.3578, lossG=-0.0808]\n",
      "Epoch 127/200: 100%|██████████| 16/16 [00:07<00:00,  2.08batch/s, lossD=1.0749, lossG=0.5345]\n",
      "Epoch 128/200: 100%|██████████| 16/16 [00:07<00:00,  2.09batch/s, lossD=1.4100, lossG=0.8596]\n",
      "Epoch 129/200: 100%|██████████| 16/16 [00:07<00:00,  2.09batch/s, lossD=1.4124, lossG=0.5915] \n",
      "Epoch 130/200: 100%|██████████| 16/16 [00:07<00:00,  2.09batch/s, lossD=1.1533, lossG=1.0904] \n",
      "Epoch 131/200: 100%|██████████| 16/16 [00:07<00:00,  2.08batch/s, lossD=1.6889, lossG=0.9141] \n",
      "Epoch 132/200: 100%|██████████| 16/16 [00:07<00:00,  2.08batch/s, lossD=1.1181, lossG=0.5226]\n",
      "Epoch 133/200: 100%|██████████| 16/16 [00:07<00:00,  2.08batch/s, lossD=1.1683, lossG=1.3087]\n",
      "Epoch 134/200: 100%|██████████| 16/16 [00:07<00:00,  2.08batch/s, lossD=1.1023, lossG=0.9881] \n",
      "Epoch 135/200: 100%|██████████| 16/16 [00:07<00:00,  2.08batch/s, lossD=1.2273, lossG=0.3896]\n",
      "Epoch 136/200: 100%|██████████| 16/16 [00:07<00:00,  2.08batch/s, lossD=0.9864, lossG=0.7765] \n",
      "Epoch 137/200: 100%|██████████| 16/16 [00:07<00:00,  2.08batch/s, lossD=0.8612, lossG=0.9671] \n",
      "Epoch 138/200: 100%|██████████| 16/16 [00:07<00:00,  2.09batch/s, lossD=1.1396, lossG=1.1720] \n",
      "Epoch 139/200: 100%|██████████| 16/16 [00:07<00:00,  2.09batch/s, lossD=1.1382, lossG=0.5504]\n",
      "Epoch 140/200: 100%|██████████| 16/16 [00:07<00:00,  2.08batch/s, lossD=0.9591, lossG=1.0459]\n",
      "Epoch 141/200: 100%|██████████| 16/16 [00:07<00:00,  2.09batch/s, lossD=1.3579, lossG=1.9913]\n",
      "Epoch 142/200: 100%|██████████| 16/16 [00:07<00:00,  2.08batch/s, lossD=0.8798, lossG=1.8141]\n",
      "Epoch 143/200: 100%|██████████| 16/16 [00:07<00:00,  2.09batch/s, lossD=1.1059, lossG=1.7990] \n",
      "Epoch 144/200: 100%|██████████| 16/16 [00:07<00:00,  2.09batch/s, lossD=0.9887, lossG=1.5330]\n",
      "Epoch 145/200: 100%|██████████| 16/16 [00:07<00:00,  2.08batch/s, lossD=0.8146, lossG=1.0094]\n",
      "Epoch 146/200: 100%|██████████| 16/16 [00:07<00:00,  2.10batch/s, lossD=1.0991, lossG=1.9320]\n",
      "Epoch 147/200: 100%|██████████| 16/16 [00:07<00:00,  2.09batch/s, lossD=0.9579, lossG=2.1400]\n",
      "Epoch 148/200: 100%|██████████| 16/16 [00:07<00:00,  2.09batch/s, lossD=0.8668, lossG=2.0034]\n",
      "Epoch 149/200: 100%|██████████| 16/16 [00:07<00:00,  2.09batch/s, lossD=0.9259, lossG=1.1353]\n",
      "Epoch 150/200: 100%|██████████| 16/16 [00:07<00:00,  2.09batch/s, lossD=0.9194, lossG=1.7972]\n",
      "Epoch 151/200: 100%|██████████| 16/16 [00:07<00:00,  2.09batch/s, lossD=0.8833, lossG=0.7807]\n",
      "Epoch 152/200: 100%|██████████| 16/16 [00:07<00:00,  2.10batch/s, lossD=0.8918, lossG=0.7275]\n",
      "Epoch 153/200: 100%|██████████| 16/16 [00:07<00:00,  2.09batch/s, lossD=0.7842, lossG=1.8555]\n",
      "Epoch 154/200: 100%|██████████| 16/16 [00:07<00:00,  2.10batch/s, lossD=1.0292, lossG=2.5997]\n",
      "Epoch 155/200: 100%|██████████| 16/16 [00:07<00:00,  2.09batch/s, lossD=0.6836, lossG=2.3424]\n",
      "Epoch 156/200: 100%|██████████| 16/16 [00:07<00:00,  2.09batch/s, lossD=0.7020, lossG=2.1152]\n",
      "Epoch 157/200: 100%|██████████| 16/16 [00:07<00:00,  2.10batch/s, lossD=0.6468, lossG=1.0296]\n",
      "Epoch 158/200: 100%|██████████| 16/16 [00:07<00:00,  2.10batch/s, lossD=0.7337, lossG=2.4271]\n",
      "Epoch 159/200: 100%|██████████| 16/16 [00:07<00:00,  2.10batch/s, lossD=0.9247, lossG=1.8465]\n",
      "Epoch 160/200: 100%|██████████| 16/16 [00:07<00:00,  2.09batch/s, lossD=0.8611, lossG=1.9237]\n",
      "Epoch 161/200: 100%|██████████| 16/16 [00:07<00:00,  2.10batch/s, lossD=0.4986, lossG=1.4186]\n",
      "Epoch 162/200: 100%|██████████| 16/16 [00:07<00:00,  2.10batch/s, lossD=0.8561, lossG=0.9493]\n",
      "Epoch 163/200: 100%|██████████| 16/16 [00:07<00:00,  2.09batch/s, lossD=0.7468, lossG=1.0918]\n",
      "Epoch 164/200: 100%|██████████| 16/16 [00:07<00:00,  2.10batch/s, lossD=0.8063, lossG=1.1171]\n",
      "Epoch 165/200: 100%|██████████| 16/16 [00:07<00:00,  2.10batch/s, lossD=0.7816, lossG=0.8621]\n",
      "Epoch 166/200: 100%|██████████| 16/16 [00:07<00:00,  2.10batch/s, lossD=0.7763, lossG=1.0677]\n",
      "Epoch 167/200: 100%|██████████| 16/16 [00:07<00:00,  2.09batch/s, lossD=0.7838, lossG=0.9158]\n",
      "Epoch 168/200: 100%|██████████| 16/16 [00:07<00:00,  2.10batch/s, lossD=0.6740, lossG=2.5859]\n",
      "Epoch 169/200: 100%|██████████| 16/16 [00:07<00:00,  2.10batch/s, lossD=0.7452, lossG=2.3861]\n",
      "Epoch 170/200: 100%|██████████| 16/16 [00:07<00:00,  2.09batch/s, lossD=0.5033, lossG=1.4119]\n",
      "Epoch 171/200: 100%|██████████| 16/16 [00:07<00:00,  2.10batch/s, lossD=0.3228, lossG=1.3165]\n",
      "Epoch 172/200: 100%|██████████| 16/16 [00:07<00:00,  2.09batch/s, lossD=0.5129, lossG=1.7438]\n",
      "Epoch 173/200: 100%|██████████| 16/16 [00:07<00:00,  2.10batch/s, lossD=0.4164, lossG=1.8351]\n",
      "Epoch 174/200: 100%|██████████| 16/16 [00:07<00:00,  2.10batch/s, lossD=0.6328, lossG=2.0321]\n",
      "Epoch 175/200: 100%|██████████| 16/16 [00:07<00:00,  2.10batch/s, lossD=0.6112, lossG=2.4019]\n",
      "Epoch 176/200: 100%|██████████| 16/16 [00:07<00:00,  2.10batch/s, lossD=0.4183, lossG=2.2044]\n",
      "Epoch 177/200: 100%|██████████| 16/16 [00:07<00:00,  2.11batch/s, lossD=0.4393, lossG=1.7494]\n",
      "Epoch 178/200: 100%|██████████| 16/16 [00:07<00:00,  2.11batch/s, lossD=0.5055, lossG=1.1832]\n",
      "Epoch 179/200: 100%|██████████| 16/16 [00:07<00:00,  2.11batch/s, lossD=0.4007, lossG=2.0897]\n",
      "Epoch 180/200: 100%|██████████| 16/16 [00:07<00:00,  2.11batch/s, lossD=0.5183, lossG=2.2050]\n",
      "Epoch 181/200: 100%|██████████| 16/16 [00:07<00:00,  2.11batch/s, lossD=0.5464, lossG=1.2115]\n",
      "Epoch 182/200: 100%|██████████| 16/16 [00:07<00:00,  2.10batch/s, lossD=0.5271, lossG=1.0190]\n",
      "Epoch 183/200: 100%|██████████| 16/16 [00:07<00:00,  2.11batch/s, lossD=0.3519, lossG=1.0494]\n",
      "Epoch 184/200: 100%|██████████| 16/16 [00:07<00:00,  2.10batch/s, lossD=0.5097, lossG=2.3821]\n",
      "Epoch 185/200: 100%|██████████| 16/16 [00:07<00:00,  2.11batch/s, lossD=0.3401, lossG=1.5281]\n",
      "Epoch 186/200: 100%|██████████| 16/16 [00:07<00:00,  2.11batch/s, lossD=0.3529, lossG=2.0631]\n",
      "Epoch 187/200: 100%|██████████| 16/16 [00:07<00:00,  2.11batch/s, lossD=0.3597, lossG=2.1272]\n",
      "Epoch 188/200: 100%|██████████| 16/16 [00:07<00:00,  2.11batch/s, lossD=0.4701, lossG=2.2585]\n",
      "Epoch 189/200: 100%|██████████| 16/16 [00:07<00:00,  2.11batch/s, lossD=0.3773, lossG=2.2566]\n",
      "Epoch 190/200: 100%|██████████| 16/16 [00:07<00:00,  2.10batch/s, lossD=0.1917, lossG=1.8684]\n",
      "Epoch 191/200: 100%|██████████| 16/16 [00:07<00:00,  2.11batch/s, lossD=0.2371, lossG=1.8303]\n",
      "Epoch 192/200: 100%|██████████| 16/16 [00:07<00:00,  2.11batch/s, lossD=0.3140, lossG=1.8486]\n",
      "Epoch 193/200: 100%|██████████| 16/16 [00:07<00:00,  2.11batch/s, lossD=0.6168, lossG=1.2501]\n",
      "Epoch 194/200: 100%|██████████| 16/16 [00:07<00:00,  2.11batch/s, lossD=0.4256, lossG=1.6034]\n",
      "Epoch 195/200: 100%|██████████| 16/16 [00:07<00:00,  2.11batch/s, lossD=0.2442, lossG=1.1051]\n",
      "Epoch 196/200: 100%|██████████| 16/16 [00:07<00:00,  2.11batch/s, lossD=0.3205, lossG=1.9956]\n",
      "Epoch 197/200: 100%|██████████| 16/16 [00:07<00:00,  2.11batch/s, lossD=0.7946, lossG=2.4269]\n",
      "Epoch 198/200: 100%|██████████| 16/16 [00:07<00:00,  2.11batch/s, lossD=0.4927, lossG=1.3003]\n",
      "Epoch 199/200: 100%|██████████| 16/16 [00:07<00:00,  2.11batch/s, lossD=0.0844, lossG=2.1422]\n",
      "Epoch 200/200: 100%|██████████| 16/16 [00:07<00:00,  2.11batch/s, lossD=0.5631, lossG=2.6764]\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "\n",
    "batch_size = 64\n",
    "image_size = 128\n",
    "nc = 3 \n",
    "nz = 100 \n",
    "ngf = 64 \n",
    "ndf = 64 \n",
    "n_epochs = 200\n",
    "lr = 2e-4 \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.CenterCrop(image_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*nc, [0.5]*nc),\n",
    "])\n",
    "\n",
    "class SingleFolderDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, folder, transform=None):\n",
    "        self.paths = [os.path.join(folder, f) for f in os.listdir(folder)\n",
    "                      if f.lower().endswith((\".png\",\".jpg\",\".jpeg\"))]\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.paths[idx]).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, 0\n",
    "\n",
    "dataset = SingleFolderDataset(data_root, transform=transform)\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True,\n",
    "                     num_workers=4, pin_memory=True)\n",
    "\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if 'Conv' in classname:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif 'BatchNorm' in classname:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "\n",
    "netG = Generator(nz, ngf, nc).to(device)\n",
    "netD = Discriminator(nc, ndf).to(device)\n",
    "netG.apply(weights_init)\n",
    "netD.apply(weights_init)\n",
    "\n",
    "\n",
    "optimG = optim.Adam(netG.parameters(), lr=lr, betas=(0.0, 0.9))\n",
    "optimD = optim.Adam(netD.parameters(), lr=lr, betas=(0.0, 0.9))\n",
    "\n",
    "\n",
    "fixed_noise = torch.randn(batch_size, nz, 1,1, device=device)\n",
    "os.makedirs(f\"{object_folder}_samples\", exist_ok=True)\n",
    "os.makedirs(f\"{object_folder}_pths\", exist_ok=True)\n",
    "\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    prog = tqdm(loader, desc=f\"Epoch {epoch}/{n_epochs}\", unit=\"batch\")\n",
    "    for imgs, _ in prog:\n",
    "        real = imgs.to(device)\n",
    "        bsz = real.size(0)\n",
    "        noise = torch.randn(bsz, nz,1,1, device=device)\n",
    "        fake = netG(noise)\n",
    "\n",
    "        optimD.zero_grad()\n",
    "        real_logits = netD(real)\n",
    "        fake_logits = netD(fake.detach())\n",
    "        lossD = torch.mean(F.relu(1. - real_logits)) + torch.mean(F.relu(1. + fake_logits))\n",
    "        lossD.backward()\n",
    "        optimD.step()\n",
    "\n",
    "        optimG.zero_grad()\n",
    "        gen_logits = netD(netG(noise))\n",
    "        lossG = -torch.mean(gen_logits)\n",
    "        lossG.backward()\n",
    "        optimG.step()\n",
    "\n",
    "        prog.set_postfix(lossD=f\"{lossD.item():.4f}\", lossG=f\"{lossG.item():.4f}\")\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        with torch.no_grad():\n",
    "            grid = utils.make_grid(netG(fixed_noise), padding=2, normalize=True)\n",
    "            utils.save_image(grid, f\"{object_folder}_samples/epoch_{epoch:03d}.png\")\n",
    "        torch.save(netG.state_dict(), f\"{object_folder}_pths/netG_epoch_{epoch}.pth\")\n",
    "        torch.save(netD.state_dict(), f\"{object_folder}_pths/netD_epoch_{epoch}.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b8c798",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_42995/3996790451.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(f\"{object_folder}_pths/netG_epoch_200.pth\", map_location=device)\n",
      "/tmp/ipykernel_42995/3996790451.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(f\"{object_folder}_pths/netD_epoch_200.pth\", map_location=device)\n",
      "Epoch 1/100: 100%|██████████| 16/16 [00:07<00:00,  2.21batch/s, lossD=0.5582, lossG=0.3672]\n",
      "Epoch 2/100: 100%|██████████| 16/16 [00:07<00:00,  2.20batch/s, lossD=0.6361, lossG=0.8199]\n",
      "Epoch 3/100: 100%|██████████| 16/16 [00:07<00:00,  2.18batch/s, lossD=0.3815, lossG=1.2214]\n",
      "Epoch 4/100: 100%|██████████| 16/16 [00:07<00:00,  2.18batch/s, lossD=0.8129, lossG=1.7784]\n",
      "Epoch 5/100: 100%|██████████| 16/16 [00:07<00:00,  2.17batch/s, lossD=0.5332, lossG=2.4298]\n",
      "Epoch 6/100: 100%|██████████| 16/16 [00:07<00:00,  2.17batch/s, lossD=0.7262, lossG=2.4343]\n",
      "Epoch 7/100: 100%|██████████| 16/16 [00:07<00:00,  2.16batch/s, lossD=0.3472, lossG=1.6165]\n",
      "Epoch 8/100: 100%|██████████| 16/16 [00:07<00:00,  2.16batch/s, lossD=0.4027, lossG=1.6045]\n",
      "Epoch 9/100: 100%|██████████| 16/16 [00:07<00:00,  2.15batch/s, lossD=0.4588, lossG=2.3816]\n",
      "Epoch 10/100: 100%|██████████| 16/16 [00:07<00:00,  2.15batch/s, lossD=0.4416, lossG=2.7604]\n",
      "Epoch 11/100: 100%|██████████| 16/16 [00:07<00:00,  2.15batch/s, lossD=0.4698, lossG=2.2914]\n",
      "Epoch 12/100: 100%|██████████| 16/16 [00:07<00:00,  2.16batch/s, lossD=0.4831, lossG=2.4542]\n",
      "Epoch 13/100: 100%|██████████| 16/16 [00:07<00:00,  2.16batch/s, lossD=0.4472, lossG=2.6415]\n",
      "Epoch 14/100: 100%|██████████| 16/16 [00:07<00:00,  2.16batch/s, lossD=0.2867, lossG=1.5336]\n",
      "Epoch 15/100: 100%|██████████| 16/16 [00:07<00:00,  2.15batch/s, lossD=0.8408, lossG=3.1583]\n",
      "Epoch 16/100: 100%|██████████| 16/16 [00:07<00:00,  2.15batch/s, lossD=0.3348, lossG=1.3756]\n",
      "Epoch 17/100: 100%|██████████| 16/16 [00:07<00:00,  2.15batch/s, lossD=0.3000, lossG=1.3806]\n",
      "Epoch 18/100: 100%|██████████| 16/16 [00:07<00:00,  2.15batch/s, lossD=0.3562, lossG=1.1232]\n",
      "Epoch 19/100: 100%|██████████| 16/16 [00:07<00:00,  2.15batch/s, lossD=0.2678, lossG=1.6913]\n",
      "Epoch 20/100: 100%|██████████| 16/16 [00:07<00:00,  2.15batch/s, lossD=0.4934, lossG=2.1710]\n",
      "Epoch 21/100: 100%|██████████| 16/16 [00:07<00:00,  2.15batch/s, lossD=0.3994, lossG=2.1150]\n",
      "Epoch 22/100: 100%|██████████| 16/16 [00:07<00:00,  2.15batch/s, lossD=0.4946, lossG=1.2291]\n",
      "Epoch 23/100: 100%|██████████| 16/16 [00:07<00:00,  2.15batch/s, lossD=0.5590, lossG=2.6673]\n",
      "Epoch 24/100: 100%|██████████| 16/16 [00:07<00:00,  2.15batch/s, lossD=0.4439, lossG=1.9447]\n",
      "Epoch 25/100: 100%|██████████| 16/16 [00:07<00:00,  2.15batch/s, lossD=0.1848, lossG=1.4611]\n",
      "Epoch 26/100: 100%|██████████| 16/16 [00:07<00:00,  2.15batch/s, lossD=0.2705, lossG=1.9545]\n",
      "Epoch 27/100: 100%|██████████| 16/16 [00:07<00:00,  2.15batch/s, lossD=0.2370, lossG=1.8843]\n",
      "Epoch 28/100: 100%|██████████| 16/16 [00:07<00:00,  2.14batch/s, lossD=0.3083, lossG=1.2798]\n",
      "Epoch 29/100: 100%|██████████| 16/16 [00:07<00:00,  2.15batch/s, lossD=0.1887, lossG=1.4697]\n",
      "Epoch 30/100: 100%|██████████| 16/16 [00:07<00:00,  2.15batch/s, lossD=0.4654, lossG=1.0852]\n",
      "Epoch 31/100: 100%|██████████| 16/16 [00:07<00:00,  2.15batch/s, lossD=0.4358, lossG=2.4886]\n",
      "Epoch 32/100: 100%|██████████| 16/16 [00:07<00:00,  2.14batch/s, lossD=0.4094, lossG=2.6854]\n",
      "Epoch 33/100: 100%|██████████| 16/16 [00:07<00:00,  2.14batch/s, lossD=0.4604, lossG=2.5458]\n",
      "Epoch 34/100: 100%|██████████| 16/16 [00:07<00:00,  2.14batch/s, lossD=0.5003, lossG=2.9851]\n",
      "Epoch 35/100: 100%|██████████| 16/16 [00:07<00:00,  2.14batch/s, lossD=0.4461, lossG=2.3433]\n",
      "Epoch 36/100: 100%|██████████| 16/16 [00:07<00:00,  2.15batch/s, lossD=0.2303, lossG=1.2705]\n",
      "Epoch 37/100: 100%|██████████| 16/16 [00:07<00:00,  2.15batch/s, lossD=0.3970, lossG=1.5993]\n",
      "Epoch 38/100: 100%|██████████| 16/16 [00:07<00:00,  2.15batch/s, lossD=0.2674, lossG=0.9625]\n",
      "Epoch 39/100: 100%|██████████| 16/16 [00:07<00:00,  2.14batch/s, lossD=0.2066, lossG=1.8732]\n",
      "Epoch 40/100: 100%|██████████| 16/16 [00:07<00:00,  2.15batch/s, lossD=0.4456, lossG=2.4683]\n",
      "Epoch 41/100: 100%|██████████| 16/16 [00:07<00:00,  2.15batch/s, lossD=0.2257, lossG=1.3237]\n",
      "Epoch 42/100: 100%|██████████| 16/16 [00:07<00:00,  2.15batch/s, lossD=0.1545, lossG=1.3117]\n",
      "Epoch 43/100: 100%|██████████| 16/16 [00:07<00:00,  2.14batch/s, lossD=0.3246, lossG=0.9517]\n",
      "Epoch 44/100: 100%|██████████| 16/16 [00:07<00:00,  2.16batch/s, lossD=0.4770, lossG=0.6942]\n",
      "Epoch 45/100: 100%|██████████| 16/16 [00:07<00:00,  2.15batch/s, lossD=0.2964, lossG=0.9762]\n",
      "Epoch 46/100: 100%|██████████| 16/16 [00:07<00:00,  2.15batch/s, lossD=0.1998, lossG=1.5013]\n",
      "Epoch 47/100: 100%|██████████| 16/16 [00:07<00:00,  2.15batch/s, lossD=0.3617, lossG=1.9716]\n",
      "Epoch 48/100: 100%|██████████| 16/16 [00:07<00:00,  2.15batch/s, lossD=0.3147, lossG=2.6100]\n",
      "Epoch 49/100: 100%|██████████| 16/16 [00:07<00:00,  2.15batch/s, lossD=0.2318, lossG=1.7573]\n",
      "Epoch 50/100: 100%|██████████| 16/16 [00:07<00:00,  2.15batch/s, lossD=0.2040, lossG=2.0775]\n",
      "Epoch 51/100: 100%|██████████| 16/16 [00:07<00:00,  2.15batch/s, lossD=0.4715, lossG=2.4247]\n",
      "Epoch 52/100: 100%|██████████| 16/16 [00:07<00:00,  2.15batch/s, lossD=0.2306, lossG=2.3746]\n",
      "Epoch 53/100: 100%|██████████| 16/16 [00:07<00:00,  2.16batch/s, lossD=0.1867, lossG=2.3683]\n",
      "Epoch 54/100: 100%|██████████| 16/16 [00:07<00:00,  2.15batch/s, lossD=0.3812, lossG=2.6494]\n",
      "Epoch 55/100: 100%|██████████| 16/16 [00:07<00:00,  2.15batch/s, lossD=0.1327, lossG=1.6762]\n",
      "Epoch 56/100: 100%|██████████| 16/16 [00:07<00:00,  2.15batch/s, lossD=0.1948, lossG=2.3262]\n",
      "Epoch 57/100: 100%|██████████| 16/16 [00:07<00:00,  2.15batch/s, lossD=0.5023, lossG=2.9761]\n",
      "Epoch 58/100: 100%|██████████| 16/16 [00:07<00:00,  2.15batch/s, lossD=0.3563, lossG=2.2459]\n",
      "Epoch 59/100: 100%|██████████| 16/16 [00:07<00:00,  2.14batch/s, lossD=0.1839, lossG=1.1486]\n",
      "Epoch 60/100: 100%|██████████| 16/16 [00:07<00:00,  2.15batch/s, lossD=0.4594, lossG=3.3341]\n",
      "Epoch 61/100: 100%|██████████| 16/16 [00:07<00:00,  2.15batch/s, lossD=0.1512, lossG=1.7296]\n",
      "Epoch 62/100: 100%|██████████| 16/16 [00:07<00:00,  2.15batch/s, lossD=0.1429, lossG=2.1166]\n",
      "Epoch 63/100: 100%|██████████| 16/16 [00:07<00:00,  2.15batch/s, lossD=0.2881, lossG=2.8175]\n",
      "Epoch 64/100: 100%|██████████| 16/16 [00:07<00:00,  2.16batch/s, lossD=0.1244, lossG=1.6672]\n",
      "Epoch 65/100: 100%|██████████| 16/16 [00:07<00:00,  2.15batch/s, lossD=0.3596, lossG=1.2142]\n",
      "Epoch 66/100: 100%|██████████| 16/16 [00:07<00:00,  2.15batch/s, lossD=0.1466, lossG=1.7886]\n",
      "Epoch 67/100: 100%|██████████| 16/16 [00:07<00:00,  2.15batch/s, lossD=0.2505, lossG=2.3347]\n",
      "Epoch 68/100: 100%|██████████| 16/16 [00:07<00:00,  2.16batch/s, lossD=0.1532, lossG=2.8165]\n",
      "Epoch 69/100: 100%|██████████| 16/16 [00:07<00:00,  2.15batch/s, lossD=0.2620, lossG=1.8409]\n",
      "Epoch 70/100: 100%|██████████| 16/16 [00:07<00:00,  2.15batch/s, lossD=0.1652, lossG=1.7087]\n",
      "Epoch 71/100: 100%|██████████| 16/16 [00:07<00:00,  2.15batch/s, lossD=0.2696, lossG=2.7089]\n",
      "Epoch 72/100: 100%|██████████| 16/16 [00:07<00:00,  2.16batch/s, lossD=0.1796, lossG=1.2462]\n",
      "Epoch 73/100: 100%|██████████| 16/16 [00:07<00:00,  2.15batch/s, lossD=0.2437, lossG=1.1596]\n",
      "Epoch 74/100: 100%|██████████| 16/16 [00:07<00:00,  2.15batch/s, lossD=0.3004, lossG=1.1389]\n",
      "Epoch 75/100: 100%|██████████| 16/16 [00:07<00:00,  2.16batch/s, lossD=0.1650, lossG=1.0551]\n",
      "Epoch 76/100: 100%|██████████| 16/16 [00:07<00:00,  2.16batch/s, lossD=0.1921, lossG=2.0119]\n",
      "Epoch 77/100: 100%|██████████| 16/16 [00:07<00:00,  2.16batch/s, lossD=0.2006, lossG=1.9491]\n",
      "Epoch 78/100: 100%|██████████| 16/16 [00:07<00:00,  2.15batch/s, lossD=0.2328, lossG=1.9732]\n",
      "Epoch 79/100: 100%|██████████| 16/16 [00:07<00:00,  2.14batch/s, lossD=0.1156, lossG=2.4405]\n",
      "Epoch 80/100: 100%|██████████| 16/16 [00:07<00:00,  2.16batch/s, lossD=0.0898, lossG=1.3982]\n",
      "Epoch 81/100: 100%|██████████| 16/16 [00:07<00:00,  2.15batch/s, lossD=0.1866, lossG=1.7853]\n",
      "Epoch 82/100: 100%|██████████| 16/16 [00:07<00:00,  2.16batch/s, lossD=0.1065, lossG=1.3992]\n",
      "Epoch 83/100: 100%|██████████| 16/16 [00:07<00:00,  2.16batch/s, lossD=0.4758, lossG=1.1540]\n",
      "Epoch 84/100: 100%|██████████| 16/16 [00:07<00:00,  2.15batch/s, lossD=0.2999, lossG=1.2013]\n",
      "Epoch 85/100: 100%|██████████| 16/16 [00:07<00:00,  2.16batch/s, lossD=0.1382, lossG=1.3755]\n",
      "Epoch 86/100: 100%|██████████| 16/16 [00:07<00:00,  2.15batch/s, lossD=0.0714, lossG=2.3990]\n",
      "Epoch 87/100: 100%|██████████| 16/16 [00:07<00:00,  2.15batch/s, lossD=0.0638, lossG=1.4368]\n",
      "Epoch 88/100: 100%|██████████| 16/16 [00:07<00:00,  2.16batch/s, lossD=0.1166, lossG=2.1470]\n",
      "Epoch 89/100: 100%|██████████| 16/16 [00:07<00:00,  2.16batch/s, lossD=0.0279, lossG=1.5372]\n",
      "Epoch 90/100: 100%|██████████| 16/16 [00:07<00:00,  2.16batch/s, lossD=0.0761, lossG=1.6665]\n",
      "Epoch 91/100: 100%|██████████| 16/16 [00:07<00:00,  2.16batch/s, lossD=0.1863, lossG=2.5063]\n",
      "Epoch 92/100: 100%|██████████| 16/16 [00:07<00:00,  2.16batch/s, lossD=0.1278, lossG=1.9992]\n",
      "Epoch 93/100: 100%|██████████| 16/16 [00:07<00:00,  2.16batch/s, lossD=0.1487, lossG=2.7288]\n",
      "Epoch 94/100: 100%|██████████| 16/16 [00:07<00:00,  2.15batch/s, lossD=0.0829, lossG=1.8198]\n",
      "Epoch 95/100: 100%|██████████| 16/16 [00:07<00:00,  2.17batch/s, lossD=0.1698, lossG=1.6781]\n",
      "Epoch 96/100: 100%|██████████| 16/16 [00:07<00:00,  2.16batch/s, lossD=0.0836, lossG=1.6079]\n",
      "Epoch 97/100: 100%|██████████| 16/16 [00:07<00:00,  2.17batch/s, lossD=0.1616, lossG=2.2495]\n",
      "Epoch 98/100: 100%|██████████| 16/16 [00:07<00:00,  2.17batch/s, lossD=0.0868, lossG=1.3981]\n",
      "Epoch 99/100: 100%|██████████| 16/16 [00:07<00:00,  2.16batch/s, lossD=0.1347, lossG=1.5238]\n",
      "Epoch 100/100: 100%|██████████| 16/16 [00:07<00:00,  2.17batch/s, lossD=0.1897, lossG=2.2451]\n"
     ]
    }
   ],
   "source": [
    "# continue training if needed\n",
    "n_epochs = 100\n",
    "\n",
    "netG = Generator(nz, ngf, nc).to(device)\n",
    "netD = Discriminator(nc, ndf).to(device)\n",
    "\n",
    "\n",
    "optimG = optim.Adam(netG.parameters(), lr=lr, betas=(0.0, 0.9))\n",
    "optimD = optim.Adam(netD.parameters(), lr=lr, betas=(0.0, 0.9))\n",
    "\n",
    "\n",
    "state_dict = torch.load(f\"{object_folder}_pths/netG_epoch_200.pth\", map_location=device)\n",
    "netG.load_state_dict(state_dict)\n",
    "state_dict = torch.load(f\"{object_folder}_pths/netD_epoch_200.pth\", map_location=device)\n",
    "netD.load_state_dict(state_dict)\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    prog = tqdm(loader, desc=f\"Epoch {epoch}/{n_epochs}\", unit=\"batch\")\n",
    "    for imgs, _ in prog:\n",
    "        real = imgs.to(device)\n",
    "        bsz = real.size(0)\n",
    "        noise = torch.randn(bsz, nz,1,1, device=device)\n",
    "        fake = netG(noise)\n",
    "\n",
    "        optimD.zero_grad()\n",
    "        real_logits = netD(real)\n",
    "        fake_logits = netD(fake.detach())\n",
    "        lossD = torch.mean(F.relu(1. - real_logits)) + torch.mean(F.relu(1. + fake_logits))\n",
    "        lossD.backward()\n",
    "        optimD.step()\n",
    "\n",
    "        optimG.zero_grad()\n",
    "        gen_logits = netD(netG(noise))\n",
    "        lossG = -torch.mean(gen_logits)\n",
    "        lossG.backward()\n",
    "        optimG.step()\n",
    "\n",
    "        prog.set_postfix(lossD=f\"{lossD.item():.4f}\", lossG=f\"{lossG.item():.4f}\")\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        with torch.no_grad():\n",
    "            grid = utils.make_grid(netG(fixed_noise), padding=2, normalize=True)\n",
    "            utils.save_image(grid, f\"{object_folder}_samples/epoch_{epoch+200:03d}.png\")\n",
    "        torch.save(netG.state_dict(), f\"{object_folder}_pths/netG_epoch_{epoch+200}.pth\")\n",
    "        torch.save(netD.state_dict(), f\"{object_folder}_pths/netD_epoch_{epoch+200}.pth\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
